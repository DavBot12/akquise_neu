# âš¡ SCRAPER SPEED OPTIMIERUNGEN

## ğŸ¯ Aktuelle Performance:
- **90-95% schneller** durch ISPRIVATE-Filter
- ~15-30 Sekunden pro Cycle (statt 8-10 Minuten)
- ~10-20 Detail-Requests (statt 720)

---

## ğŸš€ WEITERE OPTIMIERUNGEN MÃ–GLICH:

### 1. âš¡ **PARALLEL REQUESTS** (GrÃ¶ÃŸte Optimierung!)

**Aktuell (SEQUENTIELL):**
```typescript
for (const detailUrl of urls) {
  const detail = await this.fetchDetail(detailUrl);  // Warte auf jedes Detail
  // Process...
}
```
- Request 1 â†’ warten (500ms)
- Request 2 â†’ warten (500ms)
- Request 3 â†’ warten (500ms)
- **Total: 10 Ã— 500ms = 5 Sekunden**

**OPTIMIERT (PARALLEL):**
```typescript
// Batch processing mit Promise.all()
const BATCH_SIZE = 5;  // 5 parallel requests
for (let i = 0; i < urls.length; i += BATCH_SIZE) {
  const batch = urls.slice(i, i + BATCH_SIZE);
  const details = await Promise.all(
    batch.map(url => this.fetchDetail(url))
  );
  // Process all in batch...
}
```
- Request 1,2,3,4,5 â†’ parallel (500ms)
- Request 6,7,8,9,10 â†’ parallel (500ms)
- **Total: 2 Ã— 500ms = 1 Sekunde!**

**ğŸ’¡ Speedup: 5x schneller!**

---

### 2. ğŸ—œï¸ **HTTP/2 & KEEP-ALIVE** (Connection Pooling)

**Problem:** Jeder Request Ã¶ffnet neue TCP-Connection
```typescript
// Aktuell:
const response = await undiciFetch(url, { dispatcher });
// â†’ Neue Connection fÃ¼r jeden Request!
```

**LÃ¶sung: Connection Reuse**
```typescript
// Undici Agent mit Connection Pooling
const agent = new Agent({
  connections: 10,        // Max 10 gleichzeitige Connections
  pipelining: 5,          // 5 Requests per Connection
  keepAliveTimeout: 60000 // Keep alive 60s
});
```

**ğŸ’¡ Speedup: 20-30% schneller (weniger TCP Handshakes)**

---

### 3. ğŸ¯ **STREAMING PARSE** (Cheerio lazy loading)

**Problem:** Ganzes HTML wird sofort geparsed
```typescript
const $ = load(html);  // Parsed komplettes 500KB HTML
```

**LÃ¶sung: Streaming + Selective Parsing**
```typescript
// Parse nur was du brauchst
const $ = load(html, {
  decodeEntities: false,  // Nicht alle Entities decodieren
  _useHtmlParser2: true   // Schnellerer Parser
});

// Nur specific selectors
const title = $('h1').first().text();  // Stop nach erstem Match
```

**ğŸ’¡ Speedup: 10-15% schneller (weniger CPU)**

---

### 4. ğŸ’¾ **CACHING** (Schon gesehene Listings)

**Problem:** Listings werden mehrfach verarbeitet
```typescript
// Wenn Listing auf Seite 1 UND Seite 2 erscheint
```

**LÃ¶sung: In-Memory Cache**
```typescript
private seenListings = new Set<string>();

if (this.seenListings.has(listingId)) {
  continue;  // Skip bereits verarbeitet
}
this.seenListings.add(listingId);
```

**ğŸ’¡ Speedup: 5-10% schneller (weniger duplicate processing)**

---

### 5. ğŸ”¥ **EARLY ABORT** (Stop bei bekannten Listings)

**Aktuell:** Scrape bis previousFirstId
```typescript
if (listingId === categoryLastFirstId) {
  break;  // Stop pagination
}
```

**BESSER:** Stop auch bei bereits in DB
```typescript
// Check DB bevor Detail-Fetch
const exists = await this.checkListingExists(listingId);
if (exists) {
  continue;  // Skip Detail-Fetch!
}
```

**ğŸ’¡ Speedup: 30-50% bei Follow-up Scrapes**

---

### 6. âš™ï¸ **REDUCE DELAYS** (Jitter optimization)

**Aktuell:**
```typescript
await sleep(withJitter(60, 120));  // 60-180ms zwischen Listings
await sleep(withJitter(120, 80));  // 120-200ms zwischen Pages
```

**Problem:** Bei nur 10-20 Requests ist Delay unnÃ¶tig lang

**OPTIMIERT:**
```typescript
// Dynamische Delays basierend auf Request-Count
const delay = urls.length > 50
  ? withJitter(60, 120)   // Viele Requests â†’ langsamer
  : withJitter(20, 30);   // Wenige Requests â†’ schneller

await sleep(delay);
```

**ğŸ’¡ Speedup: 20-30% schneller**

---

### 7. ğŸ¨ **LAZY PHONE EXTRACTION** (On-Demand)

**Problem:** Phone wird immer extrahiert, auch wenn nicht gebraucht
```typescript
const phone = this.extractPhone(detail);  // AufwÃ¤ndig!
if (phone) {
  onPhoneFound?.({ url, phone });
}
```

**OPTIMIERT:**
```typescript
// Nur extrahieren wenn Callback existiert
if (onPhoneFound) {
  const phone = this.extractPhone(detail);
  if (phone) onPhoneFound({ url, phone });
}
```

**ğŸ’¡ Speedup: 5-10% wenn Phone nicht gebraucht wird**

---

### 8. ğŸ—„ï¸ **DATABASE BATCH INSERT** (Bulk operations)

**Problem:** Jedes Listing einzeln in DB
```typescript
for (const listing of listings) {
  await db.insert(listings).values(listing);  // Single insert
}
```

**OPTIMIERT:**
```typescript
// Batch insert alle auf einmal
await db.insert(listings).values(allListings);  // Bulk insert
```

**ğŸ’¡ Speedup: 50-70% schneller bei DB-Writes**

---

### 9. ğŸ§  **INTELLIGENT PAGINATION** (Adaptive limits)

**Aktuell:** MAX_SAFETY_PAGES = 20
```typescript
while (!foundPreviousFirstId && pageNumber <= 20)
```

**Problem:** Bei ruhigen Zeiten scrapen wir vielleicht nur 1-2 Seiten

**OPTIMIERT:**
```typescript
// Lerne von Historie
const avgPagesNeeded = this.calculateAveragePages(category);
const maxPages = Math.min(avgPagesNeeded * 1.5, 20);
```

**ğŸ’¡ Speedup: 10-20% weniger unnÃ¶tige Seiten**

---

### 10. ğŸ¯ **ISPRIVATE EARLY CHECK** (Vor Detail-Fetch)

**Aktuell:** ISPRIVATE-Filter in extractDetailUrls()
âœ… **Schon optimal!**

Aber kÃ¶nnte noch besser:
```typescript
// ISPRIVATE direkt aus Listing-Card HTML extrahieren
// OHNE komplette Seite zu laden
```

---

## ğŸ“Š **COMBINED OPTIMIZATION POTENTIAL:**

| Optimierung | Speedup | Aufwand | Risiko |
|-------------|---------|---------|--------|
| **1. Parallel Requests** | **5-10x** | Mittel | Niedrig |
| **2. Connection Pooling** | 1.3x | Niedrig | Niedrig |
| **3. Streaming Parse** | 1.15x | Niedrig | Niedrig |
| **4. Caching** | 1.1x | Niedrig | Niedrig |
| **5. Early Abort** | 1.5x | Mittel | Niedrig |
| **6. Reduce Delays** | 1.3x | Niedrig | **Mittel** (Willhaben kÃ¶nnte blocken) |
| **7. Lazy Phone** | 1.1x | Niedrig | Niedrig |
| **8. Batch DB Insert** | 1.7x | Niedrig | Niedrig |
| **9. Adaptive Pagination** | 1.2x | Mittel | Niedrig |

**TOTAL SPEEDUP: 10-30x zusÃ¤tzlich!**

Aktuell: 15-30 Sekunden
Nach Optimierung: **1-3 Sekunden!** ğŸš€

---

## ğŸ¯ **MEINE EMPFEHLUNG - TOP 3:**

### ğŸ¥‡ **1. Parallel Requests (5-10x faster)**
- Batch size: 3-5 gleichzeitig
- Mit Rate Limiting
- **SOFORT implementierbar, grÃ¶ÃŸter Gewinn**

### ğŸ¥ˆ **2. Database Batch Insert (1.7x faster)**
- Sammle alle Listings
- Ein Bulk-Insert am Ende
- **Einfach, sicher, effektiv**

### ğŸ¥‰ **3. Early Abort + Caching (1.5x faster)**
- Check DB vor Detail-Fetch
- Cache in-memory
- **Spart viele unnÃ¶tige Requests**

---

## ğŸ”§ **IMPLEMENTATION PRIORITY:**

**PHASE 1 (Quick Wins):**
1. âœ… Batch DB Insert (10 Min)
2. âœ… Lazy Phone Extraction (5 Min)
3. âœ… In-Memory Cache (10 Min)

**PHASE 2 (Medium Impact):**
4. âœ… Parallel Requests (30 Min)
5. âœ… Connection Pooling (20 Min)

**PHASE 3 (Fine-tuning):**
6. âœ… Reduce Delays (5 Min)
7. âœ… Streaming Parse (15 Min)
8. âœ… Adaptive Pagination (20 Min)

---

## âš ï¸ **WARNINGS:**

1. **Parallel Requests:** Nicht zu viele gleichzeitig (max 5)
2. **Reduce Delays:** Vorsichtig, kÃ¶nnte Willhaben triggern
3. **Connection Pooling:** Proxies mÃ¼ssen mitspielen

---

Soll ich:
1. **Parallel Requests** implementieren? (5-10x Speedup!)
2. **Batch DB Insert** implementieren? (1.7x Speedup!)
3. **Alle TOP 3** auf einmal? (Combined 10-15x Speedup!)
